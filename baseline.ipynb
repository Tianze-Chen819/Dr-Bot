{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dr.Bot Reference Demo\n",
    "\n",
    "## What to submit\n",
    "\n",
    "The link to your notebook package is automatically generated by Kaggle and will always appear in the **first cell of your notebook**, provided you’ve correctly set it up as a package. For example, in our case, the link looks like:\n",
    "\n",
    "`talhacoskun/dr-bot-baseline-model-tutorial/versions/6`\n",
    "\n",
    "This is the exact link you will submit for the competition. Please refer to the main competition page for detailed submission instructions.\n",
    "\n",
    "Because Kaggle saves all previous versions of your packages, it’s important to double-check that the link you provide corresponds to the **specific version you intend to submit**. We will evaluate **only the version you officially submit**, so please ensure it is your final, intended submission.\n",
    "\n",
    "\n",
    "\n",
    "## 1. Overview\n",
    "This notebook introduces you to fine-tuning an adapter on a Mistral model for our medical Q&A task, then packages the adapter (plus tokenizer) into a reusable Kaggle Package with a simple `Model.predict()` API.\n",
    "\n",
    "1. Uses **Add-ons → Install Dependencies** to handle libraries\n",
    "2. **Load & quantize** the base model in 4-bit to fit Kaggle’s memory constraints  \n",
    "3. **Fine-tune** on your CSV dataset of physician Q&As  \n",
    "4. **Saves** the small adapter + tokenizer into the`assets file`  \n",
    "5. **Export** a `Model` class that, on import:  \n",
    "   - downloads the official model base via `kagglehub`  \n",
    "   - applies the adapter  \n",
    "   - exposes `.predict(prompt: str) → str`\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Submission\n",
    "\n",
    "Once your package builds and your `Model.predict()` runs correctly, follow these steps to submit:\n",
    "\n",
    "1. **Save & Export Your Package**  \n",
    "   - Click **Save Version** in the Kaggle Notebook editor.  \n",
    "   - Confirm in the **Output** tab that you see a successful package build and that `package/core.py` was generated.\n",
    "   - Notebook page should say something like : package = kagglehub.package_import('talhacoskun/dr-bot-demo-notebook/versions/5')\n",
    "\n",
    "2. **Share the Notebook with Organizers**  \n",
    "   - Copy the URL of your saved notebook.\n",
    "   - Ensure your notebook is properly shared so organizers can access it (Kaggle Usernames: talhacoskun, ahsenme, derinsozen1). \n",
    "\n",
    "3. **Submit Your Entry**  \n",
    "   - In the competition’s submission form, paste the **Notebook URL**.  \n",
    "   - Organizers will import your package via `kagglehub.package_import()` and call your `Model.predict()` on the test set.  \n",
    "\n",
    "Your adapter-only package ensures the base model is downloaded automatically and your fine-tuned weights are applied at runtime. Organizers simply need your notebook link to evaluate your `Model.predict()` implementation.\n",
    "\n",
    "Afterwards, we will export your model's responses and judges will evaluate.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Example Usage:\n",
    "\n",
    "After exporting to a kaggle package, we should be able to use your model like below:\n",
    "\n",
    "```python\n",
    "import kagglehub\n",
    "package = kagglehub.package_import('notebook package link')\n",
    "model = package.Model()\n",
    "print(model.predict(\"I’ve been on amoxicillin for 3 days and my ear still hurts. Is that normal?\"))\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to fine-tuning LLMs\n",
    "This notebook serves as guide to fine tuning an LLM for the Dr. Bot competition. This notebook uses Mistral 7b Instruct and the HuggingFace transformers library, but you are free to use other models and libraries\n",
    "\n",
    "For more information on Mistral 7b: [Documentation](https://mistral.ai/news/announcing-mistral-7b)\n",
    "\n",
    "For more information of HF Transformers: [Documentation](https://huggingface.co/docs/transformers/en/index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below is needed to create a Kaggle package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#| default_exp core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Python Packages\n",
    "\n",
    "You need to import the packages that you need for your notebook. Below are the packages needed for this notebook with brief explanations.\n",
    "\n",
    "Before you can start building or training your model, you need to import the Python packages (libraries) that will give you the tools to work with models and data. Here’s a quick overview of what each one does:\n",
    "\n",
    "###  `transformers`\n",
    "This is the popular **Hugging Face library**, which gives you access to thousands of **pre-trained language models** (like GPT, BERT, T5, etc.). It’s what you’ll use to load, fine-tune, and run these models.\n",
    "\n",
    "### `datasets`\n",
    "A companion library from Hugging Face that makes it **easy to load, preprocess, and handle datasets** for training and evaluation. Supports many ready-made datasets and can also load your own data.\n",
    "\n",
    "### `accelerate`\n",
    "Helps you **train your models faster and more efficiently**, especially if you have access to multiple GPUs or TPUs. It takes care of complicated device placement and parallelization.\n",
    "\n",
    "### `bitsandbytes`\n",
    "A library that lets you do **memory-efficient training**, such as quantization (training with fewer bits). This is super helpful if you’re working with large models on limited hardware.\n",
    "\n",
    "- **To install non standard dependencies with pip, go to Add-ons -> Install Dependencies at the top of the notebook.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'kagglehub'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mkagglehub\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'kagglehub'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import kagglehub\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantization\n",
    "####  Memory Problem\n",
    "Large language models are massive because they usually store their weights as **32-bit floating-point numbers**. For example, a model with **7 billion parameters (7B)** in full precision needs about **28 GB of memory** — far too large for most consumer GPUs.\n",
    "\n",
    "####  How we solve it\n",
    "We use a technique called **quantization**, which reduces the precision from **32-bit to 4-bit**. This can **cut memory usage by up to 75%**, making it possible to run huge models on standard hardware. Quantization is now a common practice for both training and deploying large language models (LLMs).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True,bnb_4bit_compute_dtype=torch.float16,)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1\", device_map=\"auto\", quantization_config=quantization_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Dataset\n",
    "1.  Load your training data from a CSV file into a pandas DataFrame\n",
    "\n",
    "2. Convert the DataFrame into a HuggingFace Dataset the model can understand\n",
    "\n",
    "#### Importing Your Own Dataset\n",
    "\n",
    "Adding Your Dataset to Kaggle:\n",
    "Upload Dataset:\n",
    "\n",
    "1. Go to kaggle.com/datasets\n",
    "\n",
    "2. Click \"Create Dataset\"\n",
    "\n",
    "3. Upload your CSV file\n",
    "\n",
    "4. On the right side of your notebook, click `Add Input` and select your dataset\n",
    "\n",
    "```Python\n",
    "# Replace with your dataset path\n",
    "df = pd.read_csv(\"/kaggle/input/YOUR-DATASET-NAME/your_file.csv\")\n",
    "\n",
    "# Convert to HuggingFace format\n",
    "hf_ds = Dataset.from_pandas(df)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "## this is how you would use the competition training set which is already uploaded\n",
    "## change the link for manually added data\n",
    "df = pd.read_csv(\n",
    "    \"/kaggle/input/dr-bot-training-data/training_data.csv\",\n",
    "    encoding=\"latin-1\"\n",
    ")\n",
    "\n",
    "# Create HuggingFace dataset from pandas DataFrame\n",
    "hf_ds = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer\n",
    "\n",
    "Large language models don’t actually understand raw human language — they only work with **numbers**.  \n",
    "A **tokenizer** serves as a crucial translator between human-readable text and the numerical format that the model needs.\n",
    "\n",
    "- It takes your input text (like a sentence or paragraph) and breaks it down into **smaller units called tokens**. These tokens are then mapped to **unique integer IDs** that the model can process.\n",
    "- This allows the model to understand and predict patterns based on these numeric representations, rather than on the words themselves.\n",
    "\n",
    "Similarly, after the model generates predictions in the form of token IDs, the tokenizer converts these numbers **back into text** so you can read the output.\n",
    "\n",
    "In short, the tokenizer acts as a **bridge** between human language and the model’s internal numerical world — handling both the encoding (text → numbers) and decoding (numbers → text)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1\", padding_side=\"left\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.eos_token_id = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format Data For Conversation\n",
    "The Problem: Models like Mistral, ChatGPT, and Llama were trained on conversations with specific formatting. Your training data must match this exact format, or the model won't learn properly.\n",
    "\n",
    "We address this problem using `Chat Templates`, they are a standardized way to structure conversations for AI models, you can learn more [here](https://huggingface.co/learn/llm-course/en/chapter11/2)\n",
    "\n",
    "The user and assistant model our dataset as a conversation between user and the AI so that the model correctly learns what It's supposed to respond to the user with. We embed more instructions into the question to guide the LLM on what kind of answers we want. \n",
    "\n",
    "It is crucial to stay consistent in your formatting between pre-training, fine-tuning, and generation for good performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def format_conversation_batch(batch):\n",
    "    \"\"\"Format batch without system role to avoid template error\"\"\"\n",
    "    \n",
    "    formatted_texts = []\n",
    "    \n",
    "    # Define system instruction to merge with user message\n",
    "    system_instruction = \"You are an experienced primary-care physician. Provide concise, professional medical responses based on sound medical knowledge and current best practices.\"\n",
    "    \n",
    "    # Process each example in the batch\n",
    "    for i in range(len(batch['Question'])):\n",
    "        question = str(batch['Question'][i]).strip()\n",
    "        response = str(batch['Physician Response'][i]).strip()\n",
    "        \n",
    "        # Merge system instruction with user question\n",
    "        enhanced_question = f\"{system_instruction}\\n\\nQuestion: {question}\"\n",
    "        \n",
    "        # Create chat format with only user/assistant roles\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": enhanced_question},\n",
    "            {\"role\": \"assistant\", \"content\": response}\n",
    "        ]\n",
    "        \n",
    "        # Apply chat template\n",
    "        formatted_text = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=False\n",
    "        )\n",
    "        \n",
    "        formatted_texts.append(formatted_text)\n",
    "    \n",
    "    return {\"text\": formatted_texts}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing function\n",
    "\n",
    "We define a function that uses the tokenizer on our formatted data, and in the next cell we execute both steps on the datatset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    \"\"\"Tokenize the formatted text into input_ids and labels\"\"\"\n",
    "    model_inputs = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        max_length=1024,\n",
    "        truncation=True,\n",
    "        padding=False,\n",
    "        return_tensors=None\n",
    "    )\n",
    "\n",
    "    # For causal LM, labels are the same as input_ids\n",
    "    model_inputs[\"labels\"] = model_inputs[\"input_ids\"].copy()\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Apply preprocessing\n",
    "tokenized_dataset = hf_ds.map(\n",
    "    format_conversation_batch,\n",
    "    batched=True,\n",
    "    remove_columns=hf_ds.column_names,\n",
    ").map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=[\"text\"]\n",
    ")\n",
    "\n",
    "print(\"Successfully formatted dataset!\")\n",
    "print(\"Dataset size:\", len(tokenized_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Low Rank Approximation (LoRA)\n",
    "\n",
    "LoRA is a linear algebra technique that makes fine-tuning large language models practical and affordable.\n",
    "\n",
    "Instead of updating all parameters, LoRA adds small adapter layers that learn the changes needed for your specific task.\n",
    "\n",
    "We define our lora configurations here importantly setting `CAUSAL_LM` to configure it to predict the next token, a crucial LLM task\n",
    "\n",
    "You can learn more about LoRA [here](https://huggingface.co/docs/peft/main/en/conceptual_guides/lora)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "# Configure LoRA for efficient fine-tuning\n",
    "peft_config = LoraConfig(\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    inference_mode=False,\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]  # Mistral attention modules\n",
    ")\n",
    "\n",
    "# Apply LoRA to the model\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Training Arguments\n",
    "\n",
    "These define exactly how your model will learn from your data. These settings determine training speed, memory usage, and final model quality.\n",
    "\n",
    "You can learn about these in more detail [here](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./medical-mistral-7b-lora\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,\n",
    "    logging_steps=10,\n",
    "    save_steps=100,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=True,\n",
    "    warmup_ratio=0.03,\n",
    "    weight_decay=0.01,\n",
    "    dataloader_pin_memory=False,\n",
    "    remove_unused_columns=False,\n",
    "    report_to=\"none\",\n",
    "    label_names=[\"labels\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting The Dataset\n",
    "Here we split the dataset into two parts, one training and one evaluation. We train it on the larger portion and then evaluate how well it learned using a portion it hasn't seen before (evaluation split). \n",
    "\n",
    "This is to ensure that the model can work well outside of it's training set and doesn't overfit, learning to memorize the training data instead of learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split dataset\n",
    "train_indices, eval_indices = train_test_split(\n",
    "    range(len(tokenized_dataset)), \n",
    "    test_size=0.1, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "train_dataset = tokenized_dataset.select(train_indices)\n",
    "eval_dataset = tokenized_dataset.select(eval_indices)\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Compute perplexity for evaluation\"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    # For causal LM, we typically monitor loss/perplexity\n",
    "    return {}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Final Setup: Connects all your training components together and starts the actual fine-tuning process.\n",
    "\n",
    "1. Data Collator: Handles batch preparation and padding\n",
    "\n",
    "**What is a Data Collator?**\n",
    "The Data Collator takes individual tokenized examples and combines them into batches that the model can process efficiently. The data may have different lengths and the collator pads shorter examples so all examples in a batch have the same length.\n",
    "\n",
    "2. Trainer: Orchestrates the entire training process\n",
    "\n",
    "3. Training: Executes the fine-tuning loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import Trainer, DataCollatorForLanguageModeling, DataCollatorForSeq2Seq\n",
    "\n",
    "# Data collator for causal language modeling\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    label_pad_token_id=-100,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "# Create trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the Model And Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step is crucial because it **saves your fully trained model**, allowing you to reuse it later without having to rerun the entire notebook to train or export it again.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Save the fine-tuned model\n",
    "model.save_pretrained(\"./medical-mistral-7b-lora-final\", save_full_model=False)\n",
    "tokenizer.save_pretrained(\"./medical-mistral-7b-lora-final\")\n",
    "\n",
    "\n",
    "# Also save to export package\n",
    "asset_dir = kagglehub.get_package_asset_path('mistral-pretrained')\n",
    "\n",
    "model.save_pretrained(asset_dir, save_full_model=False)\n",
    "tokenizer.save_pretrained(asset_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Up Fine-Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "del(model)\n",
    "del(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True,bnb_4bit_compute_dtype=torch.float16,)\n",
    "\n",
    "basemodel = AutoModelForCausalLM.from_pretrained(\"/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1\", device_map=\"auto\", quantization_config=quantization_config)\n",
    "\n",
    "model = PeftModel.from_pretrained(\n",
    "    basemodel,\n",
    "    \"./medical-mistral-7b-lora-final\"\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./medical-mistral-7b-lora-final\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Your Fine-Tuned Model\n",
    "\n",
    "This section takes a user question and generates a medical response using your fine-tuned model.\n",
    "\n",
    "You must use the exact same format as training data, or the model won't perform optimally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def predict(prompt: str, max_new_tokens: int = 256) -> str:\n",
    "    \n",
    "    # Use the same system instruction as training for optimal results\n",
    "    system_instruction = \"You are an experienced primary-care physician. Provide concise, professional medical responses based on sound medical knowledge and current best practices.\"\n",
    "    \n",
    "    # Merge system instruction with user prompt (matching training format)\n",
    "    enhanced_prompt = f\"{system_instruction}\\n\\nQuestion: {prompt}\"\n",
    "    \n",
    "    # Create chat format with only user role (matching training data)\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": enhanced_prompt}\n",
    "    ]\n",
    "    \n",
    "    # Apply chat template with add_generation_prompt=True for inference\n",
    "    formatted_prompt = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    # Tokenize the prompt\n",
    "    inputs = tokenizer(\n",
    "        formatted_prompt, \n",
    "        return_tensors=\"pt\",\n",
    "        add_special_tokens=False\n",
    "    ).to(\"cuda\")\n",
    "    \n",
    "    # Generate response\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            repetition_penalty=1.1\n",
    "        )\n",
    "    \n",
    "    # Extract only the generated response\n",
    "    response = tokenizer.decode(\n",
    "        outputs[0][len(inputs[\"input_ids\"][0]):], \n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "    \n",
    "    return response.strip()\n",
    "\n",
    "\n",
    "# Test with a medical question\n",
    "test_question = \"My arms sometimes hurt when I sneeze? 28F, ex smoker, no drinking, 5'3, 200 lbs. Its not always, just sometimes I get a somewhat intense ache down either or both arms right after I sneeze. Should I be worried?!\"\n",
    "response = predict(test_question)\n",
    "print(f\"Question: {test_question}\")\n",
    "print(f\"Response: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting to Kaggle Package\n",
    "\n",
    "Only cells marked with #| export will be exported to the package. \n",
    "\n",
    "Here we will setup the code to initialize the model class by pulling the base model then installing our trained weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import kagglehub\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from peft import PeftModel\n",
    "\n",
    "class Model:\n",
    "    def __init__(self):\n",
    "        # 1) Download the base Mistral checkpoint\n",
    "        base_dir = kagglehub.model_download(\n",
    "            'mistral-ai/mistral/pytorch/7b-instruct-v0.1-hf/1'\n",
    "        )\n",
    "\n",
    "        # 2) Configure 4-bit quantization exactly as in training\n",
    "        quantization_config = BitsAndBytesConfig(load_in_4bit=True,bnb_4bit_compute_dtype=torch.float16,)\n",
    "\n",
    "        # 3) Load the quantized base model (auto-sharded)\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(\n",
    "            base_dir,\n",
    "            quantization_config=quantization_config,\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "\n",
    "        # 4) Load and adjust the tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            base_dir,\n",
    "            trust_remote_code=True,\n",
    "            padding_side=\"left\"\n",
    "        )\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        self.tokenizer.eos_token_id = 2\n",
    "\n",
    "        # 5) Path to pre-trained model\n",
    "        adapter_dir = kagglehub.get_package_asset_path('mistral-pretrained')\n",
    "\n",
    "        # 6) Attach the LoRA adapters on top of the base model\n",
    "        self.model = PeftModel.from_pretrained(\n",
    "            base_model,\n",
    "            adapter_dir\n",
    "        )\n",
    "\n",
    "        # 7) Eval mode + fixed seeds if desired\n",
    "        self.model.eval()\n",
    "        torch.manual_seed(42)\n",
    "        random.seed(42)\n",
    "\n",
    "    def predict(self, prompt: str, max_new_tokens: int = 256) -> str:\n",
    "        # Use your original system instruction\n",
    "        system_instruction = \"You are an experienced primary-care physician. Provide concise, professional medical responses based on sound medical knowledge and current best practices.\"\n",
    "        enhanced_prompt = f\"{system_instruction}\\n\\nQuestion: {prompt}\"\n",
    "\n",
    "        messages = [\n",
    "        {\"role\": \"user\", \"content\": enhanced_prompt}\n",
    "        ]\n",
    "\n",
    "        \n",
    "        formatted_prompt = self.tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        \n",
    "        # Tokenize & move to the correct device\n",
    "        inputs = self.tokenizer(\n",
    "        formatted_prompt, \n",
    "        return_tensors=\"pt\",\n",
    "        add_special_tokens=False\n",
    "        ).to(\"cuda\")\n",
    "\n",
    "        # Generate response\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                do_sample=False,\n",
    "                eos_token_id=self.tokenizer.eos_token_id,\n",
    "                pad_token_id=self.tokenizer.eos_token_id,\n",
    "                repetition_penalty=1.1\n",
    "            )\n",
    "\n",
    "        # Decode only the newly generated tokens and return our answer\n",
    "        response = self.tokenizer.decode(\n",
    "            outputs[0][len(inputs[\"input_ids\"][0]):], \n",
    "            skip_special_tokens=True\n",
    "        )\n",
    "        return response.strip()\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7815660,
     "sourceId": 12394315,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 1902,
     "modelInstanceId": 3900,
     "sourceId": 5112,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
